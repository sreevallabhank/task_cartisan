{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# filtering future warnings/ ignoring future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# importing keras for CNN and similar required libraries\n",
    "import keras\n",
    "from keras.layers import Input, Lambda, Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import os, random\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of classes used for training : 10\n"
     ]
    }
   ],
   "source": [
    "#configuring parameters here itself:\n",
    "IMAGE_SIZE = [30, 30, 3]\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "#path to image files directory\n",
    "train_path = '/home/sree/Music/cnntrain'\n",
    "valid_path = '/home/sree/Music/cnnvalidation'\n",
    "test_path = '/home/sree/Music/test'\n",
    "# used for getting number of files\n",
    "train_files = glob(train_path + '/*/*.jp*g')\n",
    "valid_files = glob(valid_path + '/*/*.jp*g')\n",
    "\n",
    "# used for getting number of classes\n",
    "classes = glob(train_path + '/*')\n",
    "print('no of classes used for training :',len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "set1_conv1 (Conv2D)          (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "set1_conv2 (Conv2D)          (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "set1_maxpool1 (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "set2_conv3 (Conv2D)          (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "set2_conv4 (Conv2D)          (None, 12, 12, 64)        36928     \n",
      "_________________________________________________________________\n",
      "set2_maxpool2 (MaxPooling2D) (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "fully_connected_layer1 (Dens (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "prediction_layer (Dense)     (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# developing CNN layers\n",
    "model = Sequential()\n",
    "#1st convolution set of 2 conv layers followed by max pooling has 32 filters each\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', name='set1_conv1', input_shape=IMAGE_SIZE, padding= 'same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',name='set1_conv2'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), name='set1_maxpool1'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#2nd convolution set of 2 conv layers followed by max pooling has 64 filters each\n",
    "model.add(Conv2D(64, (3, 3), padding='same',activation='relu', name='set2_conv3'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', name='set2_conv4'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), name='set2_maxpool2'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Adding fully connected layer\n",
    "model.add(Flatten(name='flatten'))\n",
    "model.add(Dense(512, activation='relu', name='fully_connected_layer1'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(classes), activation='softmax', name='prediction_layer'))\n",
    "\n",
    "# adding a optimizer\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd , metrics=['accuracy'])\n",
    "#keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2708 images belonging to 10 classes.\n",
      "Found 250 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.1, zoom_range=0.2, horizontal_flip=False, vertical_flip=False )\n",
    "valid_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "training_set = train_datagen.flow_from_directory(train_path, target_size = (30,30), batch_size = batch_size, class_mode = 'categorical')\n",
    "validation_set = valid_datagen.flow_from_directory(valid_path, target_size = (30,30), batch_size = batch_size, class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85/85 [==============================] - 37s 431ms/step - loss: 11.4470 - acc: 0.2895\n",
      "Epoch 2/30\n",
      "85/85 [==============================] - 29s 339ms/step - loss: 11.3715 - acc: 0.2945\n",
      "Epoch 3/30\n",
      "85/85 [==============================] - 45s 525ms/step - loss: 11.3715 - acc: 0.2945\n",
      "Epoch 4/30\n",
      "85/85 [==============================] - 45s 527ms/step - loss: 11.3750 - acc: 0.2943\n",
      "Epoch 5/30\n",
      "85/85 [==============================] - 44s 516ms/step - loss: 11.3715 - acc: 0.2945\n",
      "Epoch 6/30\n",
      "85/85 [==============================] - 45s 532ms/step - loss: 11.3715 - acc: 0.2945\n",
      "Epoch 7/30\n",
      "85/85 [==============================] - 44s 522ms/step - loss: 11.3750 - acc: 0.2943\n",
      "Epoch 8/30\n",
      "85/85 [==============================] - 49s 579ms/step - loss: 11.3750 - acc: 0.2943\n",
      "Epoch 9/30\n",
      "85/85 [==============================] - 46s 542ms/step - loss: 11.3575 - acc: 0.2954\n",
      "Epoch 10/30\n",
      "85/85 [==============================] - 46s 542ms/step - loss: 11.3715 - acc: 0.2945\n",
      "Epoch 11/30\n",
      "85/85 [==============================] - 49s 580ms/step - loss: 11.3750 - acc: 0.2943\n",
      "Epoch 12/30\n",
      "85/85 [==============================] - 46s 540ms/step - loss: 11.3645 - acc: 0.2949\n",
      "Epoch 13/30\n",
      "85/85 [==============================] - 55s 643ms/step - loss: 11.3680 - acc: 0.2947\n",
      "Epoch 14/30\n",
      "85/85 [==============================] - 50s 583ms/step - loss: 11.3750 - acc: 0.2943\n",
      "Epoch 15/30\n",
      "85/85 [==============================] - 47s 549ms/step - loss: 11.3715 - acc: 0.2945\n",
      "Epoch 16/30\n",
      "85/85 [==============================] - 48s 562ms/step - loss: 11.3821 - acc: 0.2938\n",
      "Epoch 17/30\n",
      "85/85 [==============================] - 54s 631ms/step - loss: 11.3680 - acc: 0.2947\n",
      "Epoch 18/30\n",
      "85/85 [==============================] - 49s 577ms/step - loss: 11.3645 - acc: 0.2949\n",
      "Epoch 19/30\n",
      "85/85 [==============================] - 56s 656ms/step - loss: 11.3610 - acc: 0.2951\n",
      "Epoch 20/30\n",
      "85/85 [==============================] - 48s 570ms/step - loss: 11.3785 - acc: 0.2941\n",
      "Epoch 21/30\n",
      "85/85 [==============================] - 50s 585ms/step - loss: 11.3645 - acc: 0.2949\n",
      "Epoch 22/30\n",
      "85/85 [==============================] - 47s 547ms/step - loss: 11.3715 - acc: 0.2945\n",
      "Epoch 23/30\n",
      "85/85 [==============================] - 50s 592ms/step - loss: 11.3715 - acc: 0.2945\n",
      "Epoch 24/30\n",
      "85/85 [==============================] - 56s 653ms/step - loss: 11.3645 - acc: 0.2949\n",
      "Epoch 25/30\n",
      "85/85 [==============================] - 59s 696ms/step - loss: 11.3680 - acc: 0.2947\n",
      "Epoch 26/30\n",
      "85/85 [==============================] - 54s 641ms/step - loss: 11.3715 - acc: 0.2945\n",
      "Epoch 27/30\n",
      "85/85 [==============================] - 59s 689ms/step - loss: 11.3785 - acc: 0.2941\n",
      "Epoch 28/30\n",
      "85/85 [==============================] - 42s 494ms/step - loss: 11.3750 - acc: 0.2943\n",
      "Epoch 29/30\n",
      "85/85 [==============================] - 41s 486ms/step - loss: 11.3645 - acc: 0.2949\n",
      "Epoch 30/30\n",
      "85/85 [==============================] - 39s 464ms/step - loss: 11.3785 - acc: 0.2941\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(training_set, steps_per_epoch=85 , epochs= epochs)\n",
    "model.save_weights('/home/sree/Documents/task_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing a random image to check our network accuracy of classification of test image data\n",
    "random_file = random.choice(os.listdir(test_path))\n",
    "file_path=test_path+'/'+random_file\n",
    "Image.open(file_path).show()\n",
    "#imagefile=file_path\n",
    "#plt.imshow('imagefile')\n",
    "test_image = image.load_img(file_path, target_size = (30, 30))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Leopards': 0, 'Motorbikes': 1, 'airplanes': 2, 'bonsai': 3, 'car_side': 4, 'chandelier': 5, 'grand_piano': 6, 'hawksbill': 7, 'ketch': 8, 'watch': 9}\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(array([1]),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label=training_set.class_indices\n",
    "print(label)\n",
    "# print the result of classifier, where test image (random) is fed to classifier to predict\n",
    "print(result)\n",
    "# result is given in the form of numpy array of class number\n",
    "result.argmax(axis=1)\n",
    "a = result[0] # Can be of any shape\n",
    "indices = np.where(a == a.max())\n",
    "print(indices)\n",
    "result[0]\n",
    "max(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
